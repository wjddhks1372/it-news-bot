import httpx
import feedparser
import logging
import asyncio

logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.sources = [
            {"name": "Karrot_Tech", "url": "https://medium.com/feed/daangn"},
            {"name": "Toss_Tech", "url": "https://toss.tech/rss.xml"},
            {"name": "AWS_News", "url": "https://aws.amazon.com/ko/blogs/aws/feed/"},
            {"name": "HackerNews", "url": "https://news.ycombinator.com/rss"},
            {"name": "Unity_Blog", "url": "https://unity.com/kr/blog/rss"},
            {"name": "GeekNews", "url": "https://news.hada.io/rss"},
            {"name": "ITWorld_Korea", "url": "https://www.itworld.co.kr/rss/feed/index.php"}
        ]
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    async def fetch_rss(self, source):
        try:
            async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=15.0) as client:
                response = await client.get(source["url"])
                if response.status_code != 200:
                    logger.error(f"âŒ {source['name']} ì‘ë‹µ ì—ëŸ¬: {response.status_code}")
                    return []
                
                feed = feedparser.parse(response.text)
                articles = []
                for entry in feed.entries[:15]:
                    articles.append({
                        "title": entry.title,
                        "link": entry.link,
                        "description": entry.get("summary", entry.get("description", "")),
                        "source": source["name"]
                    })
                logger.info(f"âœ… {source['name']} ìˆ˜ì§‘ ì„±ê³µ ({len(articles)}ê±´)")
                return articles
        except Exception as e:
            logger.error(f"âŒ {source['name']} ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return []

    async def collect_all(self):
        # 1. ìš°ì„ ìˆœìœ„ ë„ë©”ì¸ê³¼ ì¼ë°˜ ë„ë©”ì¸ ë¶„ë¦¬
        priority_names = ["Toss_Tech", "Karrot_Tech"]
        priority_sources = [s for s in self.sources if s["name"] in priority_names]
        other_sources = [s for s in self.sources if s["name"] not in priority_names]

        # 2. ë¹„ë™ê¸°ë¡œ ìˆ˜ì§‘ ì‹¤í–‰
        priority_tasks = [self.fetch_rss(s) for s in priority_sources]
        other_tasks = [self.fetch_rss(s) for s in other_sources]

        # 3. ê²°ê³¼ í•©ì¹˜ê¸° (ìš°ì„ ìˆœìœ„ ê¸°ì‚¬ê°€ ë¦¬ìŠ¤íŠ¸ ì•ìª½ì— ìœ„ì¹˜)
        priority_results = await asyncio.gather(*priority_tasks)
        other_results = await asyncio.gather(*other_tasks)

        all_articles = []
        for res in priority_results: all_articles.extend(res)
        for res in other_results: all_articles.extend(res)

        logger.info(f"ğŸš€ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê±´ (ìš°ì„ ìˆœìœ„ ë„ë©”ì¸ ìš°ì„  ë°°ì¹˜)")
        return all_articles