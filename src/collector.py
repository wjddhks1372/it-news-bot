import httpx
import feedparser
import logging
import asyncio

logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        # êµ­ë‚´ì™¸ ì£¼ìš” ê¸°ìˆ  ì†ŒìŠ¤ í™•ì¥ (P0 ìˆœìœ„ ë°˜ì˜)
        self.sources = [
            # 1ìˆœìœ„: ì„ í˜¸ ë„ë©”ì¸
            {"name": "Toss_Tech", "url": "https://toss.tech/rss.xml"},
            {"name": "Karrot_Tech", "url": "https://medium.com/feed/daangn"},
            {"name": "Naver_D2", "url": "https://d2.naver.com/d2.atom"},
            {"name": "Kakao_Tech", "url": "https://tech.kakao.com/feed/"},
            {"name": "Line_Eng", "url": "https://engineering.linecorp.com/ko/feed/"},
            {"name": "Woowahan", "url": "https://techblog.woowahan.com/feed/"},
            
            # 2ìˆœìœ„: ê¸€ë¡œë²Œ ë° ì¸í”„ë¼
            {"name": "AWS_News", "url": "https://aws.amazon.com/ko/blogs/aws/feed/"},
            {"name": "HackerNews", "url": "https://news.ycombinator.com/rss"},
            {"name": "Unity_Blog", "url": "https://unity.com/kr/blog/rss"}
        ]
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    async def fetch_rss(self, source):
        try:
            async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=20.0) as client:
                response = await client.get(source["url"])
                if response.status_code != 200:
                    logger.error(f"âŒ {source['name']} ì‘ë‹µ ì—ëŸ¬: {response.status_code}")
                    return []
                
                feed = feedparser.parse(response.text)
                articles = []
                # ê° ì†ŒìŠ¤ë‹¹ ìµœì‹  10ê±´ì”© ìˆ˜ì§‘
                for entry in feed.entries[:10]:
                    articles.append({
                        "title": entry.title,
                        "link": entry.link,
                        "description": entry.get("summary", entry.get("description", "")),
                        "source": source["name"]
                    })
                logger.info(f"âœ… {source['name']} ìˆ˜ì§‘ ì„±ê³µ ({len(articles)}ê±´)")
                return articles
        except Exception as e:
            logger.error(f"âŒ {source['name']} ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return []

    async def collect_all(self):
        # [ìš°ì„ ìˆœìœ„ ë¡œì§] ì„ í˜¸ ë„ë©”ì¸ì„ ë¦¬ìŠ¤íŠ¸ ì•ìª½ì— ë°°ì¹˜
        priority_names = ["Toss_Tech", "Karrot_Tech", "Naver_D2", "Kakao_Tech"]
        p_sources = [s for s in self.sources if s["name"] in priority_names]
        o_sources = [s for s in self.sources if s["name"] not in priority_names]

        # ë¹„ë™ê¸° ë³‘ë ¬ ìˆ˜ì§‘
        p_tasks = [self.fetch_rss(s) for s in p_sources]
        o_tasks = [self.fetch_rss(s) for s in o_sources]

        p_results = await asyncio.gather(*p_tasks)
        o_results = await asyncio.gather(*o_tasks)

        all_articles = []
        for res in p_results: all_articles.extend(res)
        for res in o_results: all_articles.extend(res)

        logger.info(f"ğŸš€ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê±´ (ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ì „ë°© ë°°ì¹˜)")
        return all_articles