import feedparser
import logging
import asyncio
import httpx # ì—¬ì „íˆ ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìš”
from urllib.parse import urlparse, urlunparse
from typing import List, Dict

logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.sources = {
            # GeekNews: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ë¶™ì—¬ ìºì‹œë¥¼ ìš°íšŒí•˜ê³  ë´‡ íƒì§€ë¥¼ íë¦½ë‹ˆë‹¤.
            "GeekNews": "https://news.hada.io/rss?v=1", 
            "ITWorld_Korea": "https://www.itworld.co.kr/rss/feed/index.php",
            "HackerNews": "https://news.ycombinator.com/rss",
            "AWS_News": "https://aws.amazon.com/ko/blogs/aws/feed/",
            "Unity_Blog": "https://unity.com/kr/blog/rss",
            "Toss_Tech": "https://toss.tech/rss.xml",
            "Karrot_Tech": "https://medium.com/feed/daangn"
        }
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.google.com/" # êµ¬ê¸€ì—ì„œ ìœ ì…ëœ ê²ƒì²˜ëŸ¼ ìœ„ì¥
        }

    def _normalize_url(self, url: str) -> str:
        parsed = urlparse(url)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))

    async def _fetch_feed(self, client: httpx.AsyncClient, name: str, url: str) -> List[Dict]:
        try:
            # ITWorld ê°™ì€ ì‚¬ì´íŠ¸ë¥¼ ìœ„í•´ ì£¼ì†Œ ëì— ìŠ¬ë˜ì‹œ ìœ ë¬´ë¥¼ ê°•ì œ ì¡°ì •í•˜ì§€ ì•ŠìŒ
            response = await client.get(url, timeout=15.0, follow_redirects=True, headers=self.headers)
            
            if response.status_code == 403 and "hada.io" in url:
                logger.warning(f"âš ï¸ {name}ê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì£¼ì†Œ ì‹œë„...")
                # 403 ë°œìƒ ì‹œ ìš°íšŒ ì£¼ì†Œë¡œ í•œ ë²ˆ ë” ì‹œë„
                response = await client.get("https://news.hada.io/rss", timeout=15.0, follow_redirects=True, headers=self.headers)

            if response.status_code != 200:
                logger.error(f"âŒ {name} ì‘ë‹µ ì—ëŸ¬: {response.status_code} ({url})")
                return []

            feed = feedparser.parse(response.text)
            articles = []
            for entry in feed.entries[:15]:
                articles.append({
                    "source": name,
                    "title": entry.get("title", "ì œëª© ì—†ìŒ").strip(),
                    "link": self._normalize_url(entry.get("link", "")),
                    "description": entry.get("description", ""),
                    "published": entry.get("published", "")
                })
            logger.info(f"âœ… {name} ìˆ˜ì§‘ ì„±ê³µ ({len(articles)}ê±´)")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ {name} ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return []

    async def collect_all(self) -> List[Dict]:
        async with httpx.AsyncClient(http2=True) as client: # HTTP/2 í™œì„±í™”ë¡œ ë´‡ íƒì§€ ìš°íšŒ í™•ë¥  ì¦ê°€
            tasks = [self._fetch_feed(client, name, url) for name, url in self.sources.items()]
            results = await asyncio.gather(*tasks)
            raw_articles = [a for sub in results for a in sub]
            
            unique_articles = {}
            for a in raw_articles:
                if a['link'] not in unique_articles:
                    unique_articles[a['link']] = a
            
            final_list = list(unique_articles.values())
            logger.info(f"ğŸš€ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(final_list)}ê±´")
            return final_list