import feedparser
import logging
import asyncio
import httpx
from urllib.parse import urlparse, urlunparse
from typing import List, Dict

logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.sources = {
            "GeekNews": "https://news.hada.io/rss",
            "ITWorld_Korea": "https://www.itworld.co.kr/rss/feed/", # ì£¼ì†Œ ìˆ˜ì •
            "HackerNews": "https://news.ycombinator.com/rss",
            "AWS_News": "https://aws.amazon.com/ko/blogs/aws/feed/",
            "Unity_Blog": "https://blog.unity.com/feed",
            "Toss_Tech": "https://toss.tech/rss.xml",
            "Karrot_Tech": "https://medium.com/feed/daangn"
        }
        # ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ê¸° ìœ„í•œ í—¤ë”
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def _normalize_url(self, url: str) -> str:
        parsed = urlparse(url)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))

    async def _fetch_feed(self, client: httpx.AsyncClient, name: str, url: str) -> List[Dict]:
        try:
            response = await client.get(url, timeout=10.0, follow_redirects=True, headers=self.headers)
            response.raise_for_status()
            feed = feedparser.parse(response.text)
            
            articles = []
            # ìµœì‹  15ê°œë§Œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ìˆ˜ì§‘ (792ê±´ ë°©ì§€)
            for entry in feed.entries[:15]:
                articles.append({
                    "source": name,
                    "title": entry.get("title", "ì œëª© ì—†ìŒ").strip(),
                    "link": self._normalize_url(entry.get("link", "")),
                    "description": entry.get("description", ""),
                    "published": entry.get("published", "")
                })
            return articles
        except Exception as e:
            logger.error(f"âŒ {name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def collect_all(self) -> List[Dict]:
        async with httpx.AsyncClient() as client:
            tasks = [self._fetch_feed(client, name, url) for name, url in self.sources.items()]
            results = await asyncio.gather(*tasks)
            raw_articles = [article for sublist in results for article in sublist]
            
            unique_articles = {}
            for article in raw_articles:
                if article['link'] not in unique_articles:
                    unique_articles[article['link']] = article
            
            final_list = list(unique_articles.values())
            logger.info(f"ğŸš€ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(final_list)}ê±´ (ìµœì‹  í•­ëª© í•œì •)")
            return final_list